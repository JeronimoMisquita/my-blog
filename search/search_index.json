{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"About Me","text":"<p>Hello my name is Jeronimo Misquita! </p>"},{"location":"blog/","title":"Blog","text":""},{"location":"blog/2026/01/01/happy-new-year-2026/","title":"Happy new year 2026!","text":"<p>Hope you have a happy and healthy new year !</p> <p> </p>"},{"location":"books/readings/","title":"Books I've read","text":"<p>2025 </p> <ul> <li>Designing Data-Intensive Applications | By Martin Kleppmann </li> </ul> <p>2024</p> <ul> <li>Certified Kubernetes Application Developer | By Benjamin Muschko </li> </ul>"},{"location":"health/groceries/","title":"\ud83d\uded2 My Shopping List","text":""},{"location":"health/groceries/#vegetables","title":"\ud83e\udd66 Vegetables","text":"<ul> <li> Tomato \u2014 6\u20138 medium</li> <li> Beans \u2014 500 g</li> <li> Mixed seasonal vegetables \u2014 2 kg</li> <li> Soup vegetables \u2014 500 g</li> <li> Salad vegetables (cucumber, carrot, onion, lettuce) \u2014 1 kg</li> <li> Leafy greens \u2014 2 bunches</li> </ul>"},{"location":"health/groceries/#fruits","title":"\ud83c\udf4e Fruits","text":"<ul> <li> Mixed fruits (apple, banana, papaya, orange, etc.) \u2014 1.5\u20132 kg</li> <li> Smoothie fruits \u2014 1 kg (4\u20135 servings)</li> </ul>"},{"location":"health/groceries/#protein-sources","title":"\ud83e\udd5a Protein Sources","text":"<ul> <li> Eggs \u2014 18\u201324 nos</li> <li> Fish \u2014 1\u20131.2 kg</li> <li> Chicken \u2014 500 g</li> <li> Dal (lentils) \u2014 500 g</li> <li> Kabuli channa (chickpeas) \u2014 500 g</li> <li> Sprouts \u2014 300 g</li> </ul>"},{"location":"health/groceries/#grains-cereals","title":"\ud83c\udf3e Grains &amp; Cereals","text":"<ul> <li> Rice \u2014 1.5 kg</li> <li> Quinoa \u2014 500 g</li> <li> Wheat flour (roti/chapati) \u2014 1.5 kg</li> <li> Gluten-free bread \u2014 1 loaf</li> <li> Oats \u2014 500 g</li> </ul>"},{"location":"health/groceries/#nuts-seeds-snacks","title":"\ud83c\udf30 Nuts, Seeds &amp; Snacks","text":"<ul> <li> Makhana (fox nuts) \u2014 250 g</li> <li> Methi seeds (fenugreek) \u2014 50 g</li> <li> Coriander seeds \u2014 50 g</li> </ul>"},{"location":"health/groceries/#beverages","title":"\ud83e\uded6 Beverages","text":"<ul> <li> Green tea \u2014 20\u201325 tea bags</li> <li> Spearmint tea \u2014 20\u201325 tea bags</li> </ul>"},{"location":"health/groceries/#pantry-miscellaneous","title":"\ud83e\uddc2 Pantry &amp; Miscellaneous","text":"<ul> <li> Organic apple cider vinegar \u2014 250 ml</li> <li> Cooking oil (olive/mustard) \u2014 250 ml</li> <li> Salt \u2014 500 g</li> <li> Turmeric powder \u2014 100 g</li> <li> Cumin seeds/powder \u2014 100 g</li> <li> Black pepper \u2014 50 g</li> <li> Curry spices \u2014 as needed</li> </ul>"},{"location":"system-design/FollowUps/","title":"Follow Ups","text":""},{"location":"system-design/FollowUps/#replication","title":"Replication","text":"<ul> <li>During selection of DB with appropriate replication strategy (couch db, cassandra, pg). How does CAP threorm fit in ?</li> <li>When to chose one replication strategy over the other ? Can i make a flow chart to show when to select one over the other ?</li> </ul>"},{"location":"system-design/HelloWorld/","title":"Hello World","text":"<p>Hi There,</p>"},{"location":"system-design/Topic%20Ideas/","title":"Topic Ideas","text":"<ul> <li>Deep dive into flight mode and analysing mem leaks and so on</li> <li>JM internals</li> <li>Node internal workings https://gemini.google.com/app/a5fdab18cca8c0f6https://gemini.google.com/app/a5fdab18cca8c0f6https://gemini.google.com/app/a5fdab18cca8c0f6</li> </ul>"},{"location":"system-design/partitioning/overview/","title":"Overview","text":"<p>We can breaking up application data into smaller chunks called partitions.  Each unique data record/document is assigned to exactly one partition. Partitions can then be assined to different nodes. Node disks size, processing power, I/O throughtput must be sufficient to process queries for its assigned partitions. Technically if partitions are small enough a node can be assigned more than one partition as well.</p> <p>Benifits of partitioning:</p> <ul> <li>Large datasets that cannot be contained on a single node can be partitioned and distributed across many nodes. <ul> <li>Individual nodes use processing power to process their own partitions. Incresing read/write throughput compared to having all data set on single node.</li> <li></li> </ul> </li> </ul>"},{"location":"system-design/replication/leaderless/","title":"Leaderless","text":""},{"location":"system-design/replication/leaderless/#overview","title":"Overview","text":"<p>In both single leader and multi-leader replication, the leader receives the writes, orders them some way and ensures changes are replicated. Followers just apply changes in the same order as the leader.</p> <p>In leaderless replication there is no leader at all. Such databases are called Dynamo Style DBs. Common products are Cassandra, Riak, etc. (Amazon DynamoDB is actually based on single-leader replication based on the Multi Paxos consensus algorithm).</p> <p>In this approach, any replica can accept write requests.</p> <p>In some implementations:</p> <ul> <li>The client sends the same write to several replicas in parallel.</li> <li>The client sends a write to one replica (coordinator) that forwards it in parallel to several other replicas. Unlike leader-based systems, the coordinator does not enforce any ordering of subsequent writes.</li> </ul> <p>Basically,</p> <ul> <li>We try to write to all nodes N. Say N=3.</li> <li>Imagine one node is down.</li> <li>Then the 2 alive nodes accept the write and the write succeeds. We simply ignore the fact that one of the nodes replied in error.</li> <li>When the node that was down comes back, it catches up with the data in one of 3 ways:<ul> <li>Read Repair: The client makes a read request to several replicas in parallel. The client notices that 1 node has stale data and updates it.</li> <li>Hinted Handoff: Another replica accepts the missed writes in the form of hints. When the replica that was supposed to receive the writes comes back, the replica storing the hints sends them to the recovered replica and then deletes the hints.</li> <li>Anti-Entropy: A background process periodically synchronizes data across replicas.</li> </ul> </li> </ul> <p>The core idea: If there exists a set of nodes N. Then you try to write to all of them in parallel, but the write succeeds if any subset W of them confirm the write operation. Similarly, a read succeeds if any R of them give some response.</p> <p>WhenW + R &gt; N, then the read or write is said to be a Strict Quorum read or Strict Quorum write.  The condition ensures that when reading there is at least one overlapping node with the written node so you will always get at least one node with the latest data. The client uses the data version number to figure out which node has the freshest data and uses that. The client can then fix data on other nodes with an additional write.</p> <p>Interestingly, the values of W and R can be set depending on your use case. If your DB has many reads and very few writes, then setting R=1 and W=N can be useful. I.e. only 1 node needs to be queried or return a value to the client (quite fast). However, if even 1 node is not working, writes cannot be applied in this config.</p> <p>In general, we set N to be an odd number (e.g 3 or 5), then set W=R=(N+1)/2. I.e. if N=3 then W=2 and R=2. One node failure is okay.</p> <p>Also in general:</p> <ul> <li>if R &lt; N, read can work if a node is down.</li> <li>if W &lt; N, write can work if a node is down.</li> <li>N=5, W=3, R=3 can tolerate 2 down nodes.</li> </ul> <p>In practice, writes and reads are sent to all replicas in parallel and values of R and W dictate how many of the N nodes need to report a success before we consider the read or write operation to be successful.</p> <p>Note: if fewer than R or W nodes return success for read and writes respectively, the operation is considered a failure.</p> <p>Note: Sharding is also possible by having shards of leaderless node groups. Therefore in practice, the number of nodes can be much more than N.</p> <p>The formula, W+R&gt;N guarantees that there is at least one node overlapping between nodes written to and read from.</p> <p>It's quite common to choose R and W values more than N/2. This not only ensures R+W &gt; N but also provides a lot of fault tolerance. I.e. at least N/2 rounded down node failures before the system becomes unusable, i.e. the system cannot provide R or W responses for read or write respectively.</p> <p>If you don't really care about always reading the latest value (some staleness is acceptable) then R and W can be much lower values. I.e. W+R &lt;= N. Here a read may not give back the latest value but the configuration provides higher availability and lower latency. Again, the system cannot read if available nodes become less than R and the system cannot be written to when the number of nodes becomes less than W.</p> <p>Select leaderless replication when:</p> <ul> <li>Prioritizing Availability: The ability for the system to continue operating and accepting writes at all times is more critical than having the absolute latest data immediately (e.g. social media feeds, logging, session stores, e-commerce shopping carts).</li> <li>Tolerance for Stale Reads: Your application can tolerate reading temporarily stale data, knowing that the data will eventually be consistent.</li> <li>High Resilience: You need strong protection against node failures and network partitions (fault tolerance).</li> <li>High Write Volume: You have a high volume of writes that need to be distributed across many nodes to achieve the required throughput.</li> </ul> <p>Disadvantages:</p> <p>Even when the W+R &gt; N condition is met, we have the following issues:</p> <ul> <li>Issue of concurrent writes. It's not clear which one happened first. If a clock is used for a tie-breaker, clock skew is a problem.</li> <li>As a write is propagating and updating nodes, a simultaneous read may hit overlapping or different nodes. It's unclear if the read sees the updated records or not while the write is in progress. It's not deterministic.</li> <li>If some nodes' write succeeds and fails on the majority of others, few successful writes are not rolled back. A subsequent quorum read may or may not return updated data.</li> <li>If a write succeeds and 3 of 5 nodes got data, it can happen that one of the 3 nodes failed and was restored by a replica without data. The number of nodes with correct data is now 2 of 5, breaking the quorum condition even though the original write succeeded.</li> </ul> <p>WARNINGS with leaderless replication: As discussed in disadvantages, even when the quorum condition is met R+W&gt;N. The system still may not always guarantee fresh data on read (due to edge cases). Leaderless replication can NOT guarantee:</p> <ul> <li>Read your own writes</li> <li>Monotonic read</li> <li>Consistent prefix reads</li> </ul> <p>Stronger guarantees need transactions or consensus (discussed later).</p>"},{"location":"system-design/replication/leaderless/#monitoring-staleness","title":"Monitoring staleness","text":"<p>In a single leader DB, the leader and replicas apply the same writes in the same order. By comparing the number of writes applied by a follower to the leader, the lag can easily be calculated. Replication lag is a very useful metric.</p> <p>In a leaderless system, staleness can be represented by the amount of hints a replica is holding for handoff. However, if the DB uses read repair then staleness can be un-ending and depends on how frequently data is accessed and discrepancies are discovered and fixed.</p>"},{"location":"system-design/replication/leaderless/#single-leader-vs-leaderless-application-performance","title":"Single leader vs Leaderless application performance","text":"<p>Single Leader</p> <ul> <li>Reads made to the leader are always consistent and return up-to-date data. However, leader read throughput can be limited. You can also make a consistent read to a sync replica.</li> <li>Reads to async replicas can return stale data.</li> <li>If the leader fails, you have to wait till it's detected and failover is completed.</li> <li>The system is very sensitive to performance problems on the leader node.</li> </ul> <p>Leaderless replication</p> <ul> <li>No Failover. However, one replica needs to detect if another one is unavailable so it can store hints about writes that the replica missed. When the replica is back, the handoff process to send hints puts additional load on the replicas to process those hints, when the system can already be under strain.</li> <li>The more replicas you have, the larger the W or R size to form strict quorums. Even if you make responses in parallel, larger R and W values increase the chance of encountering slow nodes.</li> <li>A large network disruption may make a large number of nodes unavailable to a client. The client may decide to store the writes in nodes of a different unrelated shard, hoping for handoff later to sync them (Sloppy quorum) to the destination shard. This might be a good option to prevent dropping writes. Reads made to correct shards may not see the data until handoff is completed.</li> </ul> <p>Multi Leader</p> <ul> <li>The client can communicate to the local leader (less network disruption so more reliable).</li> <li>However, writes need to be propagated across to other leaders and nodes so reads can be arbitrarily out of date if performed on replicas.</li> </ul> <p>Quorum read and write in leaderless replicaiton, provide a compromise: good fault tolerance and a high likelihood of reading up-to-date data.</p>"},{"location":"system-design/replication/leaderless/#multi-region-operation","title":"Multi Region operation","text":"<ul> <li>The client sends a write request to one node in its region. That node acts as a coordinator and forwards requests to all other nodes in the same region as well as one node in every other region. The node in the other region acts as a coordinator node and forwards data to local replicas. This avoids making cross-region requests multiple times.</li> <li>You can choose consistency levels that choose how many responses are required for a request to be successful. For example, you can request quorum across all replicas in all regions, a separate quorum in each region, or a quorum in the client's local region only. A local quorum avoids having to wait for slow requests from other regions but increases the likelihood of stale results for clients in other regions.</li> <li>Riak always prefers local quorums, and cross-region replication happens in the background in a style similar to multi-leader replication.</li> </ul>"},{"location":"system-design/replication/leaderless/#detecting-concurrent-writes","title":"Detecting concurrent writes","text":"<p>In a leaderless system, a node accepts writes from many clients even for the same key.</p> <p>Given 2 clients A and B updating some key K. Then 3 things are possible:</p> <ol> <li>A operation happens before B</li> <li>A happens after B</li> <li>A and B are concurrent</li> </ol> <p>A happens after B, means A was aware of data inserted/updated by B. I.e. if we are updating a user profile, and if B was updating the address and A was updating the name. Then after A completed, both address and name were updated because A was aware of the data changed by B.</p> <p>Concurrent (3) means that A and B are not aware of the other's operation. They don't always have to be simultaneously executing in time (although it can be the case when concurrent access happens).</p>"},{"location":"system-design/replication/leaderless/#last-write-wins-concurrency-resolution","title":"Last Write Wins concurrency resolution","text":"<p>In this approach, all clients include some increasing identifier like a timestamp. The nodes simply preserve the data inserted by the latest identifier and discard other older versions.</p> <p>Issues with this approach:</p> <ol> <li>Clock skew - 2 clients may have out-of-sync clocks, so the identifiers are not reliable and can lead to incorrect ordering.</li> <li>Data loss - If clients are updating different parts of the data concurrently e.g. one is updating the name and the other the address. Simply keeping the last write will lead to data loss of one part of the data.</li> </ol>"},{"location":"system-design/replication/leaderless/#using-version-number","title":"Using version number","text":"<p>Imagine a single node system.</p> <ul> <li>Every time you write to the DB for a key, the DB updates the value and assigns an incrementing version number to it. The write returns back the value as well as the new version number.</li> <li>The version indicates the value state inside DB the client received. The client can make further modifications and write back to the DB but it must include the version number it got in the last read. Including the version number tells the DB what past version the client knew before he made this update.</li> <li>If the DB has some newer version than what the client is writing with (maybe due to some concurrent writes from other clients in the meantime), the DB does not discard the write, it saves it as an additional value in the DB. So now the key has 2 values. The DB then in response returns both values as a write response along with an incremented version number (the version number marks that there are 2 values for the key). It is the responsibility of the client then to merge the 2 values and write back to the DB a single value.</li> </ul> <p>In this approach, all values written by concurrent updates are preserved and sent back to the client as a write response. It's the job of the client to merge them in a meaningful way.</p> <p>Note: If the value was a list of shopping cart items. Then if clients were deleting items, it's better not to physically delete them but to add a tombstone that represents the delete. During the final merging of concurrent lists, you have more information to do the merge correctly. Otherwise, you may end up adding back things that were deleted.</p> <p>In practice, each node keeps track of its own version number. The collection of versions numbers across all replicas is called a version vector.</p>"},{"location":"system-design/replication/multi-leader/","title":"Multi Leader","text":""},{"location":"system-design/replication/multi-leader/#overview","title":"Overview","text":"<p>Single leader replication has the drawback that only one node accepts writes. If that node is not reachable no writes are possible.</p> <p>Multi-leader replication is a logical extension to this idea where more than one node in the cluster accepts writes. Each leader replicates changes to all other nodes. Therefore, a leader node simultaneously also acts as a replica for other leader nodes.</p>"},{"location":"system-design/replication/multi-leader/#multi-datacenter-operation","title":"Multi datacenter operation","text":"<ul> <li>Unique flavour of multi-leader replication</li> <li>Each data center has a leader and followers.</li> <li>The leader shares updates with its local followers. The leader also replicates updates async to leaders in other datacenters.</li> <li>Leaders are responsible for conflict resolution when updates from other leaders conflict with its data representation.</li> </ul> <p>Advantages:</p> <ul> <li>The system operates even if one datacenter fails. Both writes and reads are processed.</li> <li>Lower latency on the write path as it is processed by the leader in the geographically closest datacenter. Most latency comes from the request travelling over the public low-trust internet.</li> </ul> <p>Some databases support multi-leader mode by default. But for most others, external tools are needed e.g. BDR for Postgres, Tungsten Replicator for MySQL, and Golden Gate for Oracle.</p> <p>Disadvantages:</p> <ul> <li>Leaders need a conflict resolution process.</li> <li>In many databases, multi-leader operation is a retrofitted feature and there can be surprising interactions with other database features like auto-incrementing keys, triggers, and constraints.</li> </ul> <p>NOTE: Usually multi-leader operation is avoided if possible.</p>"},{"location":"system-design/replication/multi-leader/#multi-leader-usecases","title":"Multi Leader Usecases","text":""},{"location":"system-design/replication/multi-leader/#clients-with-offline-operation","title":"Clients with offline operation","text":"<ul> <li>A use case for multi-leader replication is when the application needs to continue to work when it's disconnected from the internet.</li> <li>e.g. a calendar app on your phone. You need to be able to see your meetings (read requests) and enter new meetings (write requests) at any time even without an internet connection. The changes are synced when your device comes back online.</li> <li>This use case is like multi-datacenter operation where each device is like a datacenter and the connection between them is extremely unreliable.</li> </ul> <p>CouchDB is a document style NoSQL DB specializing in this mode of operation.</p>"},{"location":"system-design/replication/multi-leader/#collaborative-editing","title":"Collaborative editing","text":"<ul> <li>In collaborative editing (Google Docs), multiple users can edit the same document at the same time.</li> <li>Under the hood, user changes are committed to their local leader. Then they are async replicated to a central server and onwards to other users.</li> <li>A conflict resolution process is important when clashes occur (discussed later).</li> <li>In such editors, the unit of change is usually very small like a single key stroke.</li> </ul> <p>If this approach was not employed, a user would have to lock the whole document for editing and other users would have to wait when it was locked.</p>"},{"location":"system-design/replication/multi-leader/#handling-write-conflicts","title":"Handling Write conflicts","text":"<p>Since leaders accept writes to their local instances, any conflicts are detected only much later when changes are async replicated. It may be days after the user actually received a \"saved\" confirm message.</p> <p>This does not happen in single-leader systems. Here, the second write is either blocked, waits for the first one to complete, or fails with an error. The user is forced to retry the second write on top of the first one.</p>"},{"location":"system-design/replication/multi-leader/#conflict-avoidance","title":"Conflict avoidance","text":"<p>If there are resources that could be overwritten, we could design the system to ensure they are handled by the same leader. For example, a user can be assigned to a single datacenter (closest to him), and reads and writes for that user are handled by the same datacenter, so conflict is avoided across leaders. Other users may be assigned different home datacenters. This strategy avoids concurrent writes across different leaders.</p>"},{"location":"system-design/replication/multi-leader/#converging-towards-a-consistent-state","title":"Converging towards a consistent state","text":"<p>In a single leader system, if multiple writes are made to a record, the last write reflects the record state.</p> <p>In a multi-leader system, in general, there is no such ordering of writes. If replicas simply applied writes in the order they were received, different replicas may end up with a different final state (depending on the replication propagation lag of various write requests from different leaders). There must be a way that all replicas arrive at the same final value once all changes are replicated.</p> <p>Strategies for achieving convergent conflict resolution:</p> <ol> <li>Give each write a timestamp. Last timestamp wins, and other older write changes are discarded. - Strategy is prone to data loss (due to non-synced clocks on various nodes).</li> <li>Give each write a unique ID. ID is derived from the node id. During conflict, the highest ID wins (a write at a higher-numbered replica always takes precedence over writes that originated at a lower-numbered replica). The strategy is also prone to data loss.</li> <li>Somehow merge the values together. e.g. if there is a conflict in writing the title of a doc. Simply append all the possible conflicting titles together, e.g. <code>titleA/titleB</code>.</li> <li>Record the conflict in an explicit data structure that preserves all the information and write application code that resolves the conflict at some later time (perhaps by prompting the user).</li> </ol> <p>Conflicts handled via application code can be resolved in 2 ways:</p> <ol> <li>On Write - As soon as the system detects a conflict in the log of replicated changes it calls a conflict resolution handler (Bucardo in Postgres allows you to write a Perl script for this purpose). The handler cannot prompt the user. It runs as a background process and must resolve the conflict quickly.</li> <li>On Read - Details about the conflict are stored in a data structure. The structure is returned when the record is read by an application. The application will prompt the user to manually resolve the conflict and write the final result back to the DB. CouchDB works this way.</li> </ol> <p>Conflict resolution applies at the level of the record (row) or document (NoSQL DB), not the transaction level. I.e. if a transaction causes multiple writes, each write is a separate conflict that is resolved independently.</p> <p>Some research is ongoing for automatically resolving conflicts.</p> <ul> <li>Conflict-free replicated datatypes (CRDTs) - Family of data structures for sets, maps, ordered lists, counters etc. that can be concurrently edited by multiple users and automatically resolve conflict in sensible ways.</li> <li>Mergeable persistent data structures - track history explicitly similar to Git version control and use some merge functions.</li> <li>Operational transformation - designed for concurrently editing an ordered list of items such as a list of characters that constitute a text document (used in Google Docs).</li> </ul>"},{"location":"system-design/replication/multi-leader/#multi-leader-replication-topologies","title":"Multi-leader replication topologies","text":"<p>If more than one leader node exists, then replication between leaders can be organized as topologies:</p> <ol> <li>Circular - Each leader receives writes from one other leader and forwards those writes along with any of its own to the next leader in the circular chain.</li> <li>All to All - Every leader forwards writes to every other leader.</li> </ol> <p>To prevent infinite looping in (1), each node is given an identifier and replicated writes are tagged with all node IDs that they passed through. A node that sees its own ID then discards the replicated write as it applied it earlier.</p> <p>In (2) Each leader first accepts writes and saves to local disk (it can have 1 or few sync replicas in local datacenter). It then forwards to other leaders as well as other local datacenter dedicated followers the replicated changes in async mode. Network latency inside datacenter is very low so distribution inside datacenter is handled by that datacenter leader.</p> <p>Disadvantage of (1) If a node goes down in the chain, end-to-end replication stops until the faulty node is repaired. Technically, the topology can also be reconfigured to work around the fault node but it usually is a manual process.</p> <p>Disadvantage of (2): Replication paths may have different latency. Writes may appear in a random order. E.g. an update statement may appear before the insert statement it relied on. The problem is similar to the earlier problem of \"Consistent prefix reads\". To order these writes correctly a technique called version vectors is used.</p>"},{"location":"system-design/replication/overview/","title":"Overview","text":"<p>Replication involves keeping identical copies of the same data on multiple nodes/hosts.</p> <p>Entire dataset must be small enough to be entirely contained on each node.</p>"},{"location":"system-design/replication/overview/#benefits","title":"Benefits","text":"<ul> <li>Data Locality &amp; Latency: Replica nodes can be kept geographically close to users.</li> <li>Scale Read Requests: Read operations can be served by replicas. Adding more replicas can increase read throughput.</li> <li>Disaster Recovery: Replicated data acts as backup in case of disaster recovery. Improves overall dataset availability.</li> </ul>"},{"location":"system-design/replication/overview/#algorithms-for-replicating-changes","title":"Algorithms for replicating changes","text":"<ol> <li>Single Leader Replication</li> <li>Multi-Leader Replication</li> <li>Leaderless Replication</li> </ol>"},{"location":"system-design/replication/single-leader/","title":"Single Leader","text":""},{"location":"system-design/replication/single-leader/#overview","title":"Overview","text":"<p>In single leader replication</p> <ul> <li>One node is designated as the leader.</li> <li>The leader receives all write requests from clients and persists them to its local storage.</li> <li>After persisting the data, the leader propagates changes to follower nodes via a replication log or change stream.</li> <li>Followers apply writes from the leader to their local storage in the same order as processed on the leader.</li> </ul> <p>Note: Clients can only write to the leader, but may read from either the leader or any follower.</p> <p>Common Use Cases:</p> <ul> <li>Relational databases (e.g. PostgreSQL, MySQL)</li> <li>Non-relational databases (e.g. MongoDB, RethinkDB)</li> <li>Distributed message brokers (e.g. Kafka, RabbitMQ)</li> <li>Some distributed filesystems</li> </ul>"},{"location":"system-design/replication/single-leader/#synchronous-vs-asynchronous-replication","title":"Synchronous vs. Asynchronous Replication","text":"<p>Some nodes can be marked as sync replicas while others as async replicas.</p> <ul> <li>After the leader applies a write, it will wait for synchronous replicas to confirm the change before acknowledging the client.<ul> <li>Advantage: If the leader fails, a synchronous follower is guaranteed to have the latest committed data and can be promoted to leader.</li> <li>Disadvantage: If any required synchronous replica is unavailable, the system cannot accept new writes.</li> </ul> </li> <li>It is impractical for all nodes to be synchronous, as this increases write latency and reduces availability (even if one replica is down the system cant accept writes at all).</li> <li>The system may also operate with all replicas in async mode<ul> <li>Advantage: Faster write acknowledgments; the system can accept writes even if all followers are offline.</li> <li>Disadvantage: If the leader fails, writes not yet replicated to followers may be lost, meaning durability is not guaranteed even if the client received an acknowledgment.</li> </ul> </li> </ul>"},{"location":"system-design/replication/single-leader/#setting-up-new-followers","title":"Setting up new followers","text":"<p>New followers can be added to increase the number of read replicas or to replace failed ones. The process can usually be done without any downtime.</p> <p>Steps:</p> <ol> <li>Take a snapshot of the leader DB. This can usually be done without downtime (It's a common task while taking a DB backup). The snapshot also exactly correlates to one point in the leader's replication log. In PostgreSQL, this exact position is called the log sequence number, and in MySQL, it is called binlog coordinates.</li> <li>The snapshot is copied to the new follower node.</li> <li>The follower connects to the leader and requests all the changes since the snapshot was taken (delta).</li> <li>Once the follower has processed the backlog of data changes, it has caught up and begins processing data changes from the leader as they happen.</li> </ol>"},{"location":"system-design/replication/single-leader/#handling-node-outages","title":"Handling node outages","text":""},{"location":"system-design/replication/single-leader/#1-follower-failure","title":"1. Follower Failure","text":"<ul> <li>The follower disconnects temporarily from the leader, either due to a reboot (e.g. during OS upgrade) or network disruption.</li> <li>The follower usually maintains a log of data changes it processed from the leader.</li> <li>When the follower comes back, it simply looks up the last transaction that it processed and can request subsequent changes from the leader.</li> <li>Once the backlog is processed, the follower has caught up and starts processing updates from the leader as they happen.</li> </ul>"},{"location":"system-design/replication/single-leader/#2-leader-failure","title":"2. Leader Failure","text":"<p>Quite tricky. In a process called Failover:</p> <ul> <li>One of the followers (usually sync replica) is promoted to leader</li> <li>Clients need to be reconfigured to send their writes to the new leader (Either manually or intelligent routing - prefered)</li> <li>Other followers need to consume data changes from the new leader</li> </ul> <p>Failover can be:</p> <ul> <li>Initiated by admin</li> <li>Automated</li> </ul> <p>In Automated failover:</p> <ul> <li>Nodes usually send messages back and forth between each other. If any node does not respond for some timeout, it is considered dead (after 30 secs). If the leader is dead, it triggers the automated failover process.</li> <li>New leader is chosen by remaining replicas:<ul> <li>Either, the node with the latest update from the old leader is made the new leader (minimizes data loss)</li> <li>Or The next leader is chosen by a controller node (Controller node was chosen earlier, maybe when the system started)</li> </ul> </li> </ul> <p>Note: Getting all remaining nodes to agree on a new leader is a consensus problem.</p> <p>Failover is fraught with issues:</p> <ul> <li>In asynchronous replication, the new leader may not have received all writes from the old leader. What if, after the new leader is elected, the old leader comes back? Now what if the new leader has already accepted conflicting writes? You may decide to ignore/drop the writes into the old leader, but that may violate clients' durability expectations.</li> <li>Discarding writes may need to be addressed in other systems as well. In an incident at GitHub, an out-of-date MySQL follower was promoted to leader. The database system used auto-incrementing primary keys. The new leader reused primary keys previously assigned by the old leader. The primary keys were referenced in a Redis store. The reuse of primary keys caused a discrepancy between MySQL and Redis, which caused some private data to be disclosed to incorrect users.</li> <li>In certain fault scenarios, it can happen that two nodes believe they are both the leader. The situation is called split brain, and both leaders may accept writes. If there is no conflict resolution approach, data can get lost or corrupted. As a safety catch in split brain situations, one leader is forced to shut down. We must be careful not to shut down both leaders (which may happen in further error scenarios).</li> <li>Discussion exists about what should be the ideal timeout before failover is triggered. If too soon - then if the timeout happened due to an overloaded leader, unnecessary failure can just worsen the situation. If too long - longer time to recovery.</li> </ul> <p>In general, issues related to node failures, unreliable networks, trade-offs around replica consistency, durability, availability, and latency are fundamental problems in distributed systems.</p>"},{"location":"system-design/replication/single-leader/#how-is-replication-log-actually-distributed","title":"How is replication log actually distributed ?","text":""},{"location":"system-design/replication/single-leader/#1-statement-based-replication-not-used-now","title":"1. Statement based replication (not used now)","text":"<ul> <li>Leader logs statements that it executes (In relational db - Insert, update or delete) and forwards them to its followers.</li> <li>Each follower processes the statements as if it was received from a client.</li> </ul> <p>Disadvantages:</p> <ul> <li>Statements may use non-deterministic functions e.g. <code>NOW()</code> or <code>RAND()</code> that may generate different values on each replica.</li> <li>Statements need to be executed in exactly the same order on each replica. This can be challenging when there are multiple concurrently executing transactions.</li> <li>Statements that have side effects (e.g. triggers, stored procedures, user-defined functions) may generate different results unless the side effects are exactly deterministic.</li> </ul> <p>Solution:</p> <ul> <li>Leader may replace non-deterministic function calls like <code>NOW()</code> with its value before it's logged so replicas use the value directly.</li> <li>But there can be a lot of edge cases, therefore this method is not desired. Statement-based replication was used in mSQL before version 5.1. Now they use row-based replication (discussed later).</li> </ul>"},{"location":"system-design/replication/single-leader/#2-write-ahead-log-wal-shipping","title":"2. Write-Ahead log (WAL) shipping","text":"<ul> <li>Every modification is written to an append-only log file. The log is then shipped to followers.</li> <li>Followers process the log to rebuild the state on the leader.</li> </ul> <p>Disadvantage:</p> <ul> <li>The log describes data changes at a very low level. The log closely relates to the storage format the database uses.</li> <li>If the database changes its storage format from one version to another, then it is not possible to run different database versions on the leader and followers.</li> <li>Therefore, such database upgrades can require downtime when the database is not operational.</li> </ul> <p>Note: If followers and leader could run different versions - then usually followers are upgraded one at a time, then Failover is used to make one of the upgraded replicas as the leader. This is no downtime version upgrade.</p> <ul> <li>WAL replication is used by Postgres, Oracle etc.</li> </ul>"},{"location":"system-design/replication/single-leader/#3-logical-row-based-replication","title":"3. Logical (row-based) replication.","text":"<ul> <li>The system keeps a logical track of records that were inserted, deleted, or updated.</li> <li>The logs are very high level logical records and not tied to the storage engine format.</li> </ul> <p>The log is kept as follows:</p> <ul> <li>For inserted rows, the log contains new values of the columns.</li> <li>For deleted rows, the log contains enough info to uniquely identify the row that was deleted. Usually, it's the primary key logged. If there is no primary key, then old column values are logged.</li> <li>For updated rows, the log contains enough info to identify the row that changed and the new changed values.</li> <li>A transaction that modifies several rows generates several log records followed by a record indicating the transaction was committed.</li> </ul> <p>Advantages:</p> <ul> <li>The log is decoupled from storage engine internals and format; therefore, the leader and followers can run different versions.</li> <li>The logs can even be processed by external systems to build custom indexes, caches or a data warehouse. This technique is called change data capture.</li> </ul>"},{"location":"system-design/replication/single-leader/#4-trigger-based-replication","title":"4. Trigger based replication","text":"<p>Used when more flexibility is needed compared to other approaches:</p> <ol> <li>You want to replicate only a subset of the data</li> <li>You want to replicate from one database to another</li> <li>You need conflict resolution logic</li> </ol> <p>For these use cases, you want to move replication up to the application layer.</p> <ul> <li>Many relational DBs have Triggers and stored procedures. These are leveraged.</li> <li>A trigger lets you register custom application code that's executed when a data change happens (write operation). The trigger can write some data into a separate table when such changes happen. This new table can be read by an external process which can replicate the data change to another system. Databus for Oracle and Bucardo for Postgres work like this.</li> <li>Oracle GoldenGate reads database logs directly and makes data changes available for an application to consume.</li> </ul> <p>Trigger-based replication is more manual (prone to bugs, has higher setup overhead) compared to database inbuilt replication. But still useful for some cases.</p>"},{"location":"system-design/replication/single-leader/#problems-with-replication-lag","title":"Problems with Replication Lag","text":"<p>In leader-based replication, writes go to the leader and reads to followers.</p> <p>Its ideal when:</p> <ol> <li>Data pattern has a small number of writes and a large number of reads.</li> <li>Reads are scaled easily by adding more read replicas</li> </ol> <p>However, for it to work well, most replicas must be async (If all nodes were sync, single node failure may make the whole system un-usable for writes. Multiple sync replicas also add write latency).</p> <p>The problem here is that async replicas are eventually consistent. At any moment of time, a read query on a replica and the leader may give different results as the replica may be lagging behind. The term \"replication lag\" indicates how far the replica is behind the leader.</p>"},{"location":"system-design/replication/single-leader/#consistency-guarentees","title":"Consistency guarentees","text":""},{"location":"system-design/replication/single-leader/#1-reading-your-own-writes","title":"1. Reading Your Own Writes","text":"<p>\"Read after write consistency\" means:</p> <ul> <li>Users always see the data they submitted themselves.</li> <li>It does not guarantee what other users see regarding updates made by the user at any given moment.</li> </ul> <p>This consistency guarantee is an important property of a system. Without it, consider a scenario where a user submits data to the leader and then reloads the page (which may be served by a lagging replica). If the user cannot see the data they just updated, it may create the illusion that the submitted data was lost, leading to confusion and a poor user experience.</p>"},{"location":"system-design/replication/single-leader/#ideas-to-overcome-the-issue","title":"Ideas to overcome the issue:","text":"<ol> <li>You can simply read from the leader any data that the user may have modified themselves e.g. user profile info. While other users' profiles can be read from replicas.</li> <li>If most things the user can modify, then (1) may not be very useful as you will be reading most things from the leader, negating the effect of read scaling via replicas.     Another idea is to track the last user update. Then for a minute after the last update direct all read requests to the leader. Then After a minute has elapsed you can read from followers. Obviously, the assumption is that replica catchup is less than a minute. You would need to monitor replication lag and divert read traffic to replicas that have replication lag less than a minute.</li> <li>The client can note down its last update. Then include this info in its subsequent read requests. The system can ensure it serves the reads from replicas that have information that is up-to-date at least up until that point in time. The infor abou the last update may not be a timestamp. It could be a logical one like the DB replication log sequence number for the update. Not choosing a time-based value avoids problems with clock drift between frontend and backend.</li> </ol> <p>Note: If the system is distributed across geographical regions then read queries that need to go to the leader must be routed correctly to the leader datacenter.</p> <p>Note: Approach (3) that involves the client noting down and sharing the last update point may not work if the user can use many devices. A laptop client may not know the last update sequence number of the mobile client and so on.</p>"},{"location":"system-design/replication/single-leader/#2-monotonic-reads","title":"2. Monotonic reads","text":"<p>Since several replicas can be lagging by different amounts, there can be a case that if a client does multiple read requests (to different replicas) he can see things moving backwards in time.</p> <p>E.g. Imagine if an app is incrementing a counter and saving to the DB. Read requests on replicas could return:</p> <ul> <li>Query 1 - Replica 1 says count = 100 (replica lag 1s)</li> <li>Query 2 - Replica 2 says count = 50 (replica lag 100 s)</li> </ul> <p>\"Monotonic read\" guarantee means that a client may see outdated data, but subsequent read requests should never see older data than what was previously read. I.e. one cannot see data backwards in time.</p>"},{"location":"system-design/replication/single-leader/#ideas-to-overcome-the-issue_1","title":"Ideas to overcome the issue:","text":"<p>User's can always make queries to the same read replica. Different users may query different replicas. A user's replica may be chosen using a hash of the user ID rather than randomly. If a replica fails, another replica must be chosen (the replacement replica must have =&lt; lag of the one that failed).</p>"},{"location":"system-design/replication/single-leader/#3-consistent-prefix-reads","title":"3. Consistent prefix reads","text":"<p>Imagine if there are 2 DB shards. Shard 1 has a leader and followers. Shard 2 has a leader and followers. Even if \"Monotonic read\" consistency is guaranteed inside a Shard, issues can occur while querying across Shards.</p> <p>E.g. Client A writes to leader of shard 1: \"How far in future can you see Mr Cake\". Client B responds to leader of shard 2: \"10 mins in future\"</p> <p>3rd party client C needs to query both shards 1 and 2 to see the whole conversation. He queries replicas. He may see B's response before A's question.</p> <p>This issue happens because client A and client B interactions are causally related.</p> <p>\"Consistent prefix\" guarantee means if writes appear in some order, then anyone reading those writes must also read data in the same order. This issue is quite common in sharded or partitioned databases. Partitions operate independently and there is no global ordering of writes between the partitions.</p>"},{"location":"system-design/replication/single-leader/#ideas-to-overcome-the-issue_2","title":"Ideas to overcome the issue:","text":"<p>Write causally related data to the same partition. There are also algorithms (discussed later) that keep track of causal dependencies across shards/partitions.</p>"},{"location":"system-design/replication/single-leader/#solutions-to-replication-lag","title":"Solutions to replication lag","text":"<ol> <li>For some usecases large lag may not be an issue in some systems.</li> <li>For other usecases stronger guarantees can be provided by changing app behaviour as discussed in Reading your own writes, monotonic read and consistent prefix reads.</li> <li>Some databases aim to provide stronger guarantees via distributed transactions. But many distributed databases decided to abandon them claiming transactions are too expensive in terms of performance and availability and asserting that eventual consistent systems are inevitable in a scalable system.</li> </ol>"},{"location":"blog/archive/2026/","title":"2026","text":""}]}